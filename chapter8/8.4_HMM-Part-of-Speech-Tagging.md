## 8.4 HMM 词性标注（*HMM Part-of-Speech Tagging*）

在这一节中，我们将会介绍我们的第一个序列标注算法，即隐马尔可夫模型（*Hidden Markov Model*），并展示如何将其应用于词性标注。简单回顾一下，一个序列标注器的任务是为序列中的每个单元分配一个标签，从而将一个观察序列（*sequence of observations*）映射到一个相同长度的标签序列。HMM 是一个经典模型，它引入了序列建模的许多关键概念，我们将在更多现代模型中再次看到这些概念。

HMM 是一个概率序列模型：给定一个单元序列（单词、字母、语素和句子等任何东西），它会计算所有可能的标签序列服从的概率分布，并选择最佳的那个标签序列。

### 8.4.1 马克可夫链（*Markov Chains*）

HMM 是一种加强版的马尔可夫链。**马尔可夫链**（*Markov chain*）模型可以告诉我们关于随机变量或者随机*状态*（*states*）序列的概率，其中每一个值都可以从一些集合中取。这些集合可以是词、标签，或者代表任何东西的符号，例如天气。马尔可夫链做了一个非常强的假设，即如果我们想预测该序列未来的某个状态，那么唯一重要的是当前状态。在当前状态之前的所有状态都对未来没有影响，只有当前状态会影响未来状态。这就好像要预测明天的天气一样，你可以根据今天的天气来预测，但你不能根据昨天的天气。

更正式地说，考虑一个状态变量序列 $q_1,q_2,\ldots,q_i$。马尔可夫模型关于这个序列的概率的**马尔科夫假设**（*Markov assumption*）体现在：当预测未来时，过去并不重要，只有现在才重要。

$$马尔可夫假设：P(q_i = a | q_1 \ldots q_{i-1}) = P(q_i = a | q_{i-1}) \tag{8.3}$$

图 8.8a 显示了一个马尔可夫链，用于为一个天气事件序列分配概率，其中的词汇表包括 `HOT`、`COLD` 和 `WARM`。图中的节点表示状态，而边表示状态转移及其概率。转移是一种概率：从一个状态出发的所有弧线的值的和必须为 1。图 8.8b 显示了一个用于给词序列 $w_1, \ldots, w_t$ 分配概率的马尔可夫链。这个马尔可夫链想必已经很熟悉了；事实上，它表示的是一个二元语言模型（*bigram language model*），每条边表示概率 $p(w_i|w_j)$。根据图 8.8 中的两个模型，我们可以给我们词汇表中的任何序列分配一个概率。

![图 8.8](assets/fig8.8.png)

从形式上看，马尔可夫链是由以下部分组成的：

| $Q = q_1 q_2 \ldots q_N$ | $N$ 个**状态**（*states*）的集合 |
|---|---|
| $A = a_{11} a_{12} \ldots a_{N1} \ldots a_{NN}$ | 转移概率矩阵（*transition probability matrix*）$A$，每一个 $a_{ij}$ 表示从状态 $i$ 转移到状态 {j} 的概率，s.t. $\Sigma_{j=1}^{n} a_{ij} \; \forall{i}$ |
| $\pi = \pi_1, \pi_2, \ldots, \pi_N$ | 状态所服从的**初始概率分布**（*initial probability distribution*），$\pi_i$ 表示马尔可夫链从状态 {i} 开始的概率。一些状态 $j$ 可能有 $\pi_j = 0$，意味着它们不可能是初始状态。另外，$\Sigma_{i=1}^{N} \pi_i = 1$|

在继续之前，请使用图 8.8a 中的样本概率（$\pi = [0.1, 0.7, 0.2]$）来计算以下每个序列的概率：

```
(8.4)
hot hot hot hot

(8.5)
cold hot cold hot
```

这些概率的差异告诉了你一个被编码在图 8.8a 中的关于真实世界的天气事实，那么这个事实是什么？

### 8.4.2 隐马尔可夫模型（*The Hidden Markov Model*）

当我们需要计算一个观察事件序列的概率时，马尔可夫链是很有用的。然而，在许多情况下，我们感兴趣的事件是**隐藏的**（*hidden*）：我们并不能直接观察它们。例如，我们不能直接观察到文本中的词性标签。相反，我们看到的是单词，并且必须从单词序列中推断出标签。我们称这些标签为**隐藏的**，因为它们没有被观察到。

**隐马尔可夫模型**（*hidden Markov model*）（HMM）同时支持*可观察*事件（如我们在输入中所看到的词）和我们认为在概率模型中具有因果关系的*隐藏*事件（如词性标签）。HMM 是由以下几个部分组成的：

| $Q = q_1 q_2 \ldots q_N$ | $N$ 个**状态**（*states*）的集合 |
|---|---|
| $A = a_{11} a_{12} \ldots a_{N1} \ldots a_{NN}$ | **转移概率矩阵**（*transition probability matrix*）$A$，每一个 $a_{ij}$ 表示从状态 $i$ 转移到状态 {j} 的概率，s.t. $\Sigma_{j=1}^{n} a_{ij} \; \forall{i}$ |
| $O = o_1 o_2 \ldots o_T$ | 由 $T$ 个**观察值**组成的序列，每个观察值都来自一个词汇表 $V = v_1, v_2, \ldots, v_V$ |
| $B = b_i(o_t)$ | **观察值似然**（*likelihoods*）序列，也称为**发射概率**（*emission probabilities*），每一个表示从状态 $q_i$ 产生观察值 $o_t$ 的概率 |
| $\pi = \pi_1, \pi_2, \ldots, \pi_N$ | 状态所服从的**初始概率分布**（*initial probability distribution*），$\pi_i$ 表示马尔可夫链从状态 {i} 开始的概率。一些状态 $j$ 可能有 $\pi_j = 0$，意味着它们不可能是初始状态。另外，$\Sigma_{i=1}^{N} \pi_i = 1$|

一阶隐马尔可夫模型包含两个简化的假设。首先，与一阶马尔可夫链一样，一个特定状态的概率只取决于前一个状态：

$$马尔可夫假设：P(q_i | q_1 \ldots q_{i-1}) = P(q_i | q_{i-1}) \tag{8.6}$$

其次，一个输出观察值 $o_i$ 的概率只取决于产生该观察值的状态 $q_i$ ，而不取决于任何其他状态和其他观察值：

$$输出独立：P(o_i | q_1, \ldots, q_i, \ldots, q_T, o_1, \ldots, o_i, \ldots, o_T) = P(o_i | q_i) \tag{8.7}$$

### 8.4.3 HMM 标注器组件（*The components of an HMM tagger*）

让我们先看看 HMM 标注器的组成部分，然后再看如何用它来进行标注。一个 HMM 有两个组成部分，即概率 $A$ 和 $B$。

矩阵 $A$ 包含标签转移概率 $P(t_i|t_{i-1})$，表示在给定前一个标签的情况下，出现当前标签的概率。例如，情态动词后面很可能是一个基本形式的动词，即 VB，如 *race*，所以我们会期望这种概率会很高。我们通过统计一个标注语料库中第一个和第二个标签的出现次数，来计算这个转移概率的最大似然估计：

$$P(t_i|t_{i-1}) = \dfrac{C(t_{i-1}, t_i)}{C(t_{i-1})} \tag{8.8}$$

例如，在 WSJ 语料库中，MD 出现了 13124 次，其中后面跟着 VB 的情况出现了 10471 次，所以其 MLE 估计为

$$P(VB|MD) = \dfrac{C(MD, VB)}{C(MD)}=\dfrac{10471}{13124}=0.80 \tag{8.9}$$
